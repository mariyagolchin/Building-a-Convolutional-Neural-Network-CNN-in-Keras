{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building a Convolutional Neural Network (CNN) in Keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+Em0t2nKEYO8GXieOgp1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariyagolchin/Building-a-Convolutional-Neural-Network-CNN-in-Keras/blob/main/Building_a_Convolutional_Neural_Network_(CNN)_in_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vr8jwPp5G7z"
      },
      "source": [
        "## Building a Convolutional Neural Network (CNN) in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep5HT41x5EWG"
      },
      "source": [
        "https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QeJRXKZ5B58"
      },
      "source": [
        "https://github.com/eijaz1/Building-a-CNN-in-Keras-Tutorial/blob/master/cnn_tutorial.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci-Gvkalfe5g"
      },
      "source": [
        "# **Loading the dataset**\n",
        "The **mnist dataset **is conveniently provided to us as part of the **Keras library**.\n",
        " \n",
        "so we can **easily load the dataset**. \n",
        "\n",
        "Out of the **70,000 images** provided in the dataset:\n",
        "\n",
        "   ** 60,000 **are given for** training** \n",
        "   \n",
        "   and\n",
        "   \n",
        "   ** 10,000** are given for **testing**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvN6LbUgEBe"
      },
      "source": [
        "When we load the dataset below, **X_train and X_test** will contain the images, and **y_train and y_test** will contain the digits that those images represent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_Z0vi9efdm3"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUfAKt6gf4f"
      },
      "source": [
        "# Exploratory data analysis\n",
        "Now let’s take a look at one of the images in our dataset to see what we are working with. We will plot the first image in our dataset and check its size using the ‘shape’ function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "MHgjfPJigi2P",
        "outputId": "36798fb9-e52f-49a4-bafb-803230471121"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plot the first image in the dataset\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb21bf36cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCW73M0oguoW",
        "outputId": "0030710d-ed63-4d04-b482-3e2d3db0bad7"
      },
      "source": [
        "#check image shape\n",
        "X_train[0].shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfFUQFo_hKvU"
      },
      "source": [
        "## Data pre-processing\n",
        "Next, we need to reshape our dataset inputs** (X_train and X_test) **to the shape that our model expects when we train the model. The first number is the number of images **(60,000 for X_train and 10,000 for X_test)**. Then comes the shape of each image **(28x28)**. The last number is 1, which signifies that the images are greyscale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7atChnDVhVm7"
      },
      "source": [
        "#reshape data to fit model\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qNlGEoFhpxt"
      },
      "source": [
        "We need to ‘**one-hot-encode**’ our target variable. This means that** a column will be created for each output category** and a binary variable is inputted for each category. For example, we saw that the first image in the dataset is a 5. This means that the sixth number in our array will have a 1 and the rest of the array will be filled with 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA1JFtzch9hl",
        "outputId": "64b85c82-4210-4500-f947-1583a076ffb1"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_train[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L2mXiYOiNq0"
      },
      "source": [
        "## Building the model\n",
        "Now we are ready to build our model. Here is the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFZWx6SziNEc"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za1aAUhjihh8"
      },
      "source": [
        "The **model type** that we will be using is **Sequential**.\n",
        "\n",
        "**Sequential** is the **easiest way to build a model in Keras**. It allows you to **build a model layer by layer.**\n",
        "We use the **‘add()’ function** to **add layers to our model**.\n",
        "\n",
        "Our **first 2 layers** are **Conv2D layers**. These are **convolution layers** that will **deal with our input images**, which are seen as **2-dimensional matrices**.\n",
        "\n",
        "**64** in the first layer and **32** in the second layer are the** number of nodes in each layer**. This number **can be adjusted to be higher or lower**, depending on the size of the dataset. **In our case, 64 and 32 work well**, so we will stick with this for now.\n",
        "\n",
        "**Kernel size** is the **size of the filter matrix** for our convolution. So a **kernel size of 3 **means we will have a **3x3 filter matrix**. Refer back to the introduction and the first image for a refresher on this.\n",
        "\n",
        "**Activation** is the** activation function** for the layer. The activation function we will be using for our first 2 layers is the **ReLU**, or **Rectified Linear Activation**. This activation function has been proven to **work well in neural networks**.\n",
        "\n",
        "Our **first layer** also takes in an** input shape**. This is the shape of each input image, 28,28,1 as seen earlier on, with the 1 signifying that the images are greyscale.\n",
        "\n",
        "In between the **Conv2D layers and the dense layer**, there is a **‘Flatten’ layer**. Flatten serves as a **connection between the convolution and dense layers**.\n",
        "\n",
        "\n",
        "**‘Dense’** is the **layer type** we will use in for our output layer. Dense is a standard layer type that is used in many cases for neural networks.\n",
        "We will have **10 nodes in our output layer**, one for each possible outcome (0–9).\n",
        "\n",
        "The **activation is ‘softmax’**. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk2wdMU8miyL"
      },
      "source": [
        "## **Compiling the model**\n",
        "Next, we need to compile our model. Compiling the model takes three parameters: optimizer, loss and metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9df9qmZmGIw"
      },
      "source": [
        "#compile model using accuracy to measure model performance\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fra71Qutmb1_"
      },
      "source": [
        "\n",
        "The **optimizer** **controls the learning rate**. We will be using **‘adam’** as our optmizer. Adam is generally a **good optimizer to use for many cases**. The adam optimizer adjusts the learning rate throughout training.\n",
        "\n",
        "The **learning rate** determines how fast the optimal weights for the model are calculated. A **smaller learning rate may** lead to **more accurate weights** (up to a certain point), but the **time it takes to compute the weights will be longer.**\n",
        "\n",
        "We will use ‘**categorical_crossentropy’** for our **loss function**. This is the **most common choice for classification**. A **lower score indicates that the model is performing better**.\n",
        "\n",
        "To make things even easier to interpret, we will use the **‘accuracy’** metric to **see the accuracy score on the validation set when we train the model**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMb9oVmInxnQ"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Wa2tFrmbNr",
        "outputId": "a92f74a5-b0f0-4d8d-a3d9-b0bc425d18b4"
      },
      "source": [
        "#train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 354s 188ms/step - loss: 0.5187 - accuracy: 0.9260 - val_loss: 0.0589 - val_accuracy: 0.9801\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 353s 188ms/step - loss: 0.0565 - accuracy: 0.9820 - val_loss: 0.0543 - val_accuracy: 0.9822\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 350s 187ms/step - loss: 0.0385 - accuracy: 0.9875 - val_loss: 0.0592 - val_accuracy: 0.9830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb21862cad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVG-mcxhn2DL"
      },
      "source": [
        "Now we will train our model.To train, we will use the **‘fit()’ function** **on our model** with the **following parameters:** **training data (train_X), target data (train_y), validation data, and the number of epochs**.\n",
        "For our **validation data**, we will use the** test set** provided to us in our dataset, which we have **split into X_test and y_test**.\n",
        "\n",
        "**The number of epochs is the number of times the model will cycle through the data**. The more epochs we run, the more the model will improve, up to a certain point. After that point, the model will stop improving during each epoch. For our model, we will **set the number of epochs to 3**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHw4GPTR3MsU",
        "outputId": "60c78f06-f510-4db7-9ce6-3b59aeff73a9"
      },
      "source": [
        "#predict first 4 images in the test set\n",
        "model.predict(X_test[:4])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.5621191e-09, 2.2740132e-12, 1.4486498e-08, 1.5363687e-09,\n",
              "        1.3807479e-13, 2.5407156e-11, 2.3049254e-16, 1.0000000e+00,\n",
              "        1.9903233e-08, 1.4292896e-08],\n",
              "       [3.5237638e-06, 4.3844243e-06, 9.9993491e-01, 2.4215550e-09,\n",
              "        7.2854861e-10, 2.8229089e-10, 3.3519245e-05, 1.1493596e-13,\n",
              "        2.3609322e-05, 1.0350132e-11],\n",
              "       [7.2509676e-10, 9.9999988e-01, 1.9183135e-09, 1.5036394e-11,\n",
              "        4.1457216e-08, 9.9059849e-10, 1.6125068e-09, 4.5794937e-09,\n",
              "        9.0832316e-08, 5.7770708e-11],\n",
              "       [9.9975353e-01, 1.5211349e-10, 1.1443099e-07, 4.5119517e-09,\n",
              "        1.3434362e-07, 2.6669485e-07, 2.3912154e-04, 2.8662213e-09,\n",
              "        2.3319212e-06, 4.6146288e-06]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw1r6Ftt3dmr",
        "outputId": "70d3dab9-660c-45cb-c5db-9e0e7cb276c2"
      },
      "source": [
        "#actual results for first 4 images in test set\n",
        "y_test[:4]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}